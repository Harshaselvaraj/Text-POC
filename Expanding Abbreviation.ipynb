{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageNgramModel:\n",
    "    \"\"\" Remember and predict which letters usually follows which. \"\"\"\n",
    "    def __init__(self, order=1, smoothing=1.0, recursive=0.001):\n",
    "        self.order = order\n",
    "        self.smoothing = smoothing\n",
    "        self.recursive = recursive\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\" Estimate all counts on a text \"\"\"\n",
    "        self.counter_ = defaultdict(lambda: Counter())\n",
    "        self.unigrams_ = Counter()\n",
    "        self.vocabulary_ = set()\n",
    "        for i, token in enumerate(corpus[self.order:]):\n",
    "            context = corpus[i:(i+self.order)]\n",
    "            self.counter_[context][token] += 1\n",
    "            self.unigrams_[token] +=1\n",
    "            self.vocabulary_.add(token)\n",
    "        self.vocabulary_ = sorted(list(self.vocabulary_))\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            self.child_ = LanguageNgramModel(self.order-1, self.smoothing, self.recursive)\n",
    "            self.child_.fit(corpus)\n",
    "            \n",
    "    def get_counts(self, context):\n",
    "        \"\"\" Get smoothed count of each letter appearing after context \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        freq_dict = self.counter_[local]\n",
    "        freq = pd.Series(index=self.vocabulary_)\n",
    "        for i, token in enumerate(self.vocabulary_):\n",
    "            freq[token] = freq_dict[token] + self.smoothing\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            child_freq = self.child_.get_counts(context) * self.recursive\n",
    "            freq += child_freq\n",
    "        return freq\n",
    "    \n",
    "    def predict_proba(self, context):\n",
    "        \"\"\" Get smoothed probability of each letter appearing after context \"\"\"\n",
    "        counts = self.get_counts(context)\n",
    "        return counts / counts.sum()\n",
    "    \n",
    "    def single_log_proba(self, context, continuation):\n",
    "        \"\"\" Estimate log-probability that context is followed by continuation \"\"\"\n",
    "        result = 0.0\n",
    "        for token in continuation:\n",
    "            result += np.log(self.predict_proba(context)[token])\n",
    "            context += token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation):\n",
    "        \"\"\" Estimate probability that context is followed by continuation \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingLetterModel:\n",
    "    \"\"\" Remember and predict which letters are usually missing. \"\"\"\n",
    "    def __init__(self, order=0, smoothing_missed=0.3, smoothing_total=1.0):\n",
    "        self.order = order\n",
    "        self.smoothing_missed = smoothing_missed\n",
    "        self.smoothing_total = smoothing_total\n",
    "    def fit(self, sentence_pairs):\n",
    "        self.missed_counter_ = defaultdict(lambda: Counter())\n",
    "        self.total_counter_ = defaultdict(lambda: Counter())\n",
    "        for (original, observed) in sentence_pairs:\n",
    "            for i, (original_letter, observed_letter) in enumerate(zip(original[self.order:], observed[self.order:])):\n",
    "                context = original[i:(i+self.order)]\n",
    "                if observed_letter == '-':\n",
    "                    self.missed_counter_[context][original_letter] += 1\n",
    "                self.total_counter_[context][original_letter] += 1 \n",
    "    def predict_proba(self, context, last_letter):\n",
    "        \"\"\" Estimate probability that last_letter after context is missed \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        missed_freq = self.missed_counter_[local][last_letter] + self.smoothing_missed\n",
    "        total_freq = self.total_counter_[local][last_letter] + self.smoothing_total\n",
    "        return missed_freq / total_freq\n",
    "    \n",
    "    def single_log_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Estimate log-probability of continuaton being distorted to actual after context. \n",
    "        If actual is None, assume no distortion\n",
    "        \"\"\"\n",
    "        if not actual:\n",
    "            actual = continuation\n",
    "        result = 0.0\n",
    "        for orig_token, act_token in zip(continuation, actual):\n",
    "            pp = self.predict_proba(context, orig_token)\n",
    "            if act_token == '-':\n",
    "                pp = 1 - pp\n",
    "            result += np.log(pp)\n",
    "            context += orig_token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Estimate probability of continuaton being distorted to actual after context. \n",
    "        If actual is None, assume no distortion\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism=0.5, cache=None):\n",
    "    options = []\n",
    "    for letter in lang_model.vocabulary_ + ['']:\n",
    "        if letter:  # assume a missing letter\n",
    "            next_letter = letter\n",
    "            new_suffix = suffix\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log(missed_model.predict_proba(prefix, letter))\n",
    "        else:  # assume no missing letter\n",
    "            next_letter = suffix[0]\n",
    "            new_suffix = suffix[1:]\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log((1 - missed_model.predict_proba(prefix, next_letter)))\n",
    "        proba_next_letter = - np.log(lang_model.single_proba(prefix, next_letter))\n",
    "        if cache:\n",
    "            proba_suffix = cache[len(new_suffix)] * optimism\n",
    "        else:\n",
    "            proba_suffix = - np.log(lang_model.single_proba(new_prefix, new_suffix)) * optimism\n",
    "        proba = prefix_proba + proba_next_letter + proba_missing_state + proba_suffix\n",
    "        options.append((proba, new_prefix, new_suffix, letter, proba_suffix))\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_channel(word, lang_model, missed_model, freedom=1.0, max_attempts=1000, optimism=0.1, verbose=True):\n",
    "    query = word + ' '\n",
    "    prefix = ' '\n",
    "    prefix_proba = 0.0\n",
    "    suffix = query\n",
    "    full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "    no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "    best_logprob = full_origin_logprob + no_missing_logprob\n",
    "    # add empty beginning to the heap\n",
    "    heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "    # add the default option (no missing letters) to candidates\n",
    "    candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "    if verbose:\n",
    "        # todo: include distortion probability\n",
    "        print('baseline score is', best_logprob)\n",
    "    # prepare cache for suffixes (the slowest operation)\n",
    "    cache = {}\n",
    "    for i in range(len(query)+1):\n",
    "        future_suffix = query[:i]\n",
    "        cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "        cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "    \n",
    "    for i in range(max_attempts):\n",
    "        if not heap:\n",
    "            break\n",
    "        next_best = heappop(heap)\n",
    "        if verbose:\n",
    "            print(next_best)\n",
    "        if next_best[2] == '':  # it is a leaf\n",
    "            # this is the best leaf as far, add it to candidates\n",
    "            if next_best[0] <= best_logprob + freedom:\n",
    "                candidates.append(next_best)\n",
    "                # update the best likelihood\n",
    "                if next_best[0] < best_logprob:\n",
    "                    best_logprob = next_best[0]\n",
    "        else: # it is not a leaf - generate more options\n",
    "            prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "            prefix = next_best[1]\n",
    "            suffix = next_best[2]\n",
    "            new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "            # add only the solution potentioally no worse than the best + freedom\n",
    "            for new_option in new_options: \n",
    "                if new_option[0] < best_logprob + freedom:\n",
    "                    heappush(heap, new_option)\n",
    "    if verbose:\n",
    "        print('heap size is', len(heap), 'after', i, 'iterations')\n",
    "    result = {}\n",
    "    for candidate in candidates:\n",
    "        if candidate[0] <= best_logprob + freedom:\n",
    "            result[candidate[1][1:-1]] = candidate[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k plan employee benefit plan authorized by internal revenue code section k whereby an employer establishes an account for each participating employee and each participant elects to deposit a portion of his or her salary into the account the amount deposited is not subject to income tax this is the most common type of salary reduction plans  a misstatement is inconsequential if a reasonable person would conclude after considering the possibility of further undetected misstatements that the misstatement either individually or when aggregated with other misstatements would clearly be immaterial to the financial statements if a reasonable person could not reach such a conclusion regarding a particular misstatement that misstatement is more than inconsequential  abatement complete removal of an amount due usually referring to a tax abatement a penalty abatement or an interest abatement within a governing agency   absorption costing an approach to product costing that assigns a representativ\n"
     ]
    }
   ],
   "source": [
    "with open('accounting terms.txt', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "import re\n",
    "text2 = re.sub(r'[^a-z ]+', '', text.lower().replace('\\n', ' '))\n",
    "print(text2[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' abcdefghijklmnopqrstuvwxyz'\n"
     ]
    }
   ],
   "source": [
    "all_letters = ''.join(list(sorted(list(set(text2)))))\n",
    "print(repr(all_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_set = [] + [(all_letters, '-' * len(all_letters))] * 3 + [(all_letters, all_letters)] * 10 + [('aeiouy', '------')] * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -14241.407785007828\n",
      "1 -11669.625521970873\n",
      "2 -9131.981344709282\n",
      "3 -7495.287810769386\n",
      "4 -7860.915675167994\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tmp = LanguageNgramModel(i, 1.0, 0.001)\n",
    "    tmp.fit(text2[0:-5000])\n",
    "    print(i, tmp.single_log_proba(' ', text2[-5000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_lang_m = LanguageNgramModel(4, 0.001, 0.01)\n",
    "big_lang_m.fit(text2)\n",
    "big_err_m = MissingLetterModel(0, 0.1)\n",
    "big_err_m.fit(missing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'equip': 21.06765260437274,\n",
       " 'equipme': 21.488484758096885,\n",
       " 'equipment': 19.505340909496294}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('equip', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average': 12.788721504012619}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('avg', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 8.159271854013921}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('val', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'htl': 15.969590860546486,\n",
       " 'htle': 16.778589734463562,\n",
       " 'htline': 14.868173966337155,\n",
       " 'htly': 15.620779305580742}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('htl', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labor': 9.396400299087876}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('lbr', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accum': 20.32462037011524,\n",
       " 'accumular': 19.887297345300013,\n",
       " 'accumulate': 18.33715931588073,\n",
       " 'accumulated': 19.210050111412855,\n",
       " 'accumulatio': 21.062298793171916}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('accum', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discount': 11.61182032238721, 'discounts': 14.302659691836924}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('discnt', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comple': 11.488833646106656,\n",
       " 'comples': 14.380924742921977,\n",
       " 'complex': 14.351387722031802,\n",
       " 'comply': 13.045891808168188}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('compl', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_channel('misc', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11.585656396051844, '  ', 'avg', ' ', 7.651135031499331),\n",
       " (9.400374515051253, ' a', 'avg', 'a', 6.6936431004705454),\n",
       " (11.40433271613345, ' b', 'avg', 'b', 6.5958596495427955),\n",
       " (11.34257925140478, ' c', 'avg', 'c', 6.758417758051705),\n",
       " (11.537429404562896, ' d', 'avg', 'd', 6.608516115946221),\n",
       " (10.291494484875397, ' e', 'avg', 'e', 6.6785643775338155),\n",
       " (11.872940015379823, ' f', 'avg', 'f', 7.0862189107289275),\n",
       " (11.445613683041893, ' g', 'avg', 'g', 6.176649389482181),\n",
       " (11.971284468517618, ' h', 'avg', 'h', 6.62981597519682),\n",
       " (9.82111837446565, ' i', 'avg', 'i', 6.654816767194983),\n",
       " (11.085179896390887, ' j', 'avg', 'j', 5.506740204516885),\n",
       " (10.942251160093887, ' k', 'avg', 'k', 5.379425282380009),\n",
       " (11.445661223609012, ' l', 'avg', 'l', 6.300229929023635),\n",
       " (11.455320430123496, ' m', 'avg', 'm', 6.473514885004395),\n",
       " (11.91756081177267, ' n', 'avg', 'n', 6.736044162484071),\n",
       " (9.606787373925366, ' o', 'avg', 'o', 6.6116911708420005),\n",
       " (11.345943253410141, ' p', 'avg', 'p', 6.75348726781124),\n",
       " (10.942971546955375, ' q', 'avg', 'q', 5.383059632398188),\n",
       " (11.397786318589601, ' r', 'avg', 'r', 6.590911742239792),\n",
       " (11.825674791430052, ' s', 'avg', 's', 7.222330296795171),\n",
       " (11.13015909064882, ' t', 'avg', 't', 7.103573925499305),\n",
       " (9.98399581723775, ' u', 'avg', 'u', 5.950901280516796),\n",
       " (11.466313818273344, ' v', 'avg', 'v', 6.050986036007581),\n",
       " (11.418367744660891, ' w', 'avg', 'w', 6.355747737645929),\n",
       " (10.943749143583114, ' x', 'avg', 'x', 5.327053825865265),\n",
       " (9.756964852160381, ' y', 'avg', 'y', 5.479475348871379),\n",
       " (10.95173628023518, ' z', 'avg', 'z', 5.331405384555378),\n",
       " (9.106172446059428, ' a', 'vg', '', 5.288670538330804)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_options(0,' ','avg',big_lang_m,big_err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline score is 20.387755905172256\n",
      "(18.34898031465503, ' ', 'avg ', '', 18.34898031465503)\n",
      "(16.285630682073435, ' a', 'vg ', '', 12.468128774344809)\n",
      "(16.471204972362393, ' av', 'g ', '', 7.786665074547269)\n",
      "(17.315142567055396, ' ave', 'g ', 'e', 7.786665074547269)\n",
      "(17.942191427989087, ' ava', 'g ', 'a', 7.786665074547269)\n",
      "(18.066478524587417, ' avg', ' ', '', 3.1710464594368584)\n",
      "(17.726444365088188, ' avg ', '', '', -0.0)\n",
      "(18.23251967141732, ' avai', 'g ', 'i', 7.786665074547269)\n",
      "(18.85012617720313, ' aver', 'g ', 'r', 7.786665074547269)\n",
      "(18.854395868838978, ' a', 'avg ', 'a', 16.147664454258273)\n",
      "(19.142760657341636, ' o', 'avg ', 'o', 16.147664454258273)\n",
      "(19.190678827759832, ' an', 'vg ', 'n', 12.468128774344809)\n",
      "(19.27244428280196, ' avera', 'g ', 'a', 7.786665074547269)\n",
      "(15.168048418919401, ' averag', ' ', '', 3.1710464594368584)\n",
      "(15.453771090656028, ' average', ' ', 'e', 3.1710464594368584)\n",
      "(12.788721504012619, ' average ', '', '', -0.0)\n",
      "(17.217128641193476, ' average ', ' ', ' ', 3.1710464594368584)\n",
      "(19.145366617325983, ' averagec', ' ', 'c', 3.1710464594368584)\n",
      "(19.313966061528937, ' i', 'avg ', 'i', 16.147664454258273)\n",
      "(19.326916586098456, ' a ', 'vg ', ' ', 12.468128774344809)\n",
      "(19.41231588009457, ' averaged', ' ', 'd', 3.1710464594368584)\n",
      "(19.67516058121864, ' avo', 'g ', 'o', 7.786665074547269)\n",
      "(19.745836636449624, ' avail', 'g ', 'l', 7.786665074547269)\n",
      "(19.760594561599856, ' e', 'avg ', 'e', 16.147664454258273)\n",
      "(19.891566839382445, ' ac', 'vg ', 'c', 12.468128774344809)\n",
      "(19.924317357840557, ' au', 'vg ', 'u', 12.468128774344809)\n",
      "(20.00095348931959, ' as', 'vg ', 's', 12.468128774344809)\n",
      "(20.082185818810785, '  ', 'avg ', ' ', 16.147664454258273)\n",
      "(20.17424961940779, ' t', 'avg ', 't', 16.147664454258273)\n",
      "(20.18075899097923, ' u', 'avg ', 'u', 16.147664454258273)\n",
      "(20.425153957547273, ' y', 'avg ', 'y', 16.147664454258273)\n",
      "(20.73182594761135, ' c', 'avg ', 'c', 16.147664454258273)\n",
      "(20.740120439857172, ' p', 'avg ', 'p', 16.147664454258273)\n",
      "(20.75100894889315, ' s', 'avg ', 's', 16.147664454258273)\n",
      "(20.896472631700572, ' ar', 'vg ', 'r', 12.468128774344809)\n",
      "(20.93438555890917, ' f', 'avg ', 'f', 16.147664454258273)\n",
      "(20.954539030608082, ' r', 'avg ', 'r', 16.147664454258273)\n",
      "(20.956137520848927, ' b', 'avg ', 'b', 16.147664454258273)\n",
      "(21.076577742874946, ' d', 'avg ', 'd', 16.147664454258273)\n",
      "(21.129469999377374, ' m', 'avg ', 'm', 16.147664454258273)\n",
      "(21.131601782672558, ' al', 'vg ', 'l', 12.468128774344809)\n",
      "(21.169363213005838, ' at', 'vg ', 't', 12.468128774344809)\n",
      "(21.210284461273233, ' w', 'avg ', 'w', 16.147664454258273)\n",
      "(21.245974398658305, ' am', 'vg ', 'm', 12.468128774344809)\n",
      "(21.29309574884365, ' l', 'avg ', 'l', 16.147664454258273)\n",
      "(21.317986702178278, ' avge', ' ', 'e', 3.1710464594368584)\n",
      "(21.329181103546873, ' n', 'avg ', 'n', 16.147664454258273)\n",
      "(21.3410505486044, ' ai', 'vg ', 'i', 12.468128774344809)\n",
      "(21.416628747817985, ' g', 'avg ', 'g', 16.147664454258273)\n",
      "(21.489132947579073, ' h', 'avg ', 'h', 16.147664454258273)\n",
      "(21.49389658814599, ' avga', ' ', 'a', 3.1710464594368584)\n",
      "(21.562992236524035, ' v', 'avg ', 'v', 16.147664454258273)\n",
      "(21.56904433135104, ' avgo', ' ', 'o', 3.1710464594368584)\n",
      "(21.57748953861134, ' avgi', ' ', 'i', 3.1710464594368584)\n",
      "(21.674241712068127, ' ag', 'vg ', 'g', 12.468128774344809)\n",
      "(21.69183889181169, ' avi', 'g ', 'i', 7.786665074547269)\n",
      "(21.70521152891404, ' avgu', ' ', 'u', 3.1710464594368584)\n",
      "(21.70757636881546, ' q', 'avg ', 'q', 16.147664454258273)\n",
      "(21.710490331972153, ' k', 'avg ', 'k', 16.147664454258273)\n",
      "(21.726104146132275, ' j', 'avg ', 'j', 16.147664454258273)\n",
      "(21.76435977197612, ' x', 'avg ', 'x', 16.147664454258273)\n",
      "(21.767995349938076, ' z', 'avg ', 'z', 16.147664454258273)\n",
      "(21.771530135535624, ' avgy', ' ', 'y', 3.1710464594368584)\n",
      "(21.806041089979622, ' ad', 'vg ', 'd', 12.468128774344809)\n",
      "(21.933255753846787, ' ap', 'vg ', 'p', 12.468128774344809)\n",
      "(22.154851502269043, ' avg ', ' ', ' ', 3.1710464594368584)\n",
      "(22.17256400227211, ' af', 'vg ', 'f', 12.468128774344809)\n",
      "(22.386894652050003, ' ab', 'vg ', 'b', 12.468128774344809)\n",
      "(22.41002934990393, ' av', 'vg ', 'v', 12.468128774344809)\n",
      "(22.47564981399768, ' ay', 'vg ', 'y', 12.468128774344809)\n",
      "(22.56397629705316, ' ae', 'vg ', 'e', 12.468128774344809)\n",
      "(22.57786445480285, ' ave ', 'g ', ' ', 7.786665074547269)\n",
      "(22.587748710561534, ' aa', 'vg ', 'a', 12.468128774344809)\n",
      "(22.591599874809006, ' avag', ' ', '', 3.1710464594368584)\n",
      "(22.6155748456017, ' ao', 'vg ', 'o', 12.468128774344809)\n",
      "(22.62546500553436, ' avy', 'g ', 'y', 7.786665074547269)\n",
      "(22.69666694606414, ' avu', 'g ', 'u', 7.786665074547269)\n",
      "(22.781295514536907, ' avgr', ' ', 'r', 3.1710464594368584)\n",
      "(22.826766374678602, ' avgn', ' ', 'n', 3.1710464594368584)\n",
      "(22.84352712584019, ' aveg', ' ', '', 3.1710464594368584)\n",
      "(22.860693785359697, ' avgh', ' ', 'h', 3.1710464594368584)\n",
      "(22.86266262924561, ' avgt', ' ', 't', 3.1710464594368584)\n",
      "(22.869060609099925, ' avgs', ' ', 's', 3.1710464594368584)\n",
      "(22.95279529290679, ' avgl', ' ', 'l', 3.1710464594368584)\n",
      "(22.954870807113736, ' avgc', ' ', 'c', 3.1710464594368584)\n",
      "(22.964095784182202, ' avgd', ' ', 'd', 3.1710464594368584)\n",
      "(22.979652907035238, ' avgm', ' ', 'm', 3.1710464594368584)\n",
      "(22.98085163879515, ' avgp', ' ', 'p', 3.1710464594368584)\n",
      "(22.982128059188128, ' avgf', ' ', 'f', 3.1710464594368584)\n",
      "(22.989379243852692, ' avgg', ' ', 'g', 3.1710464594368584)\n",
      "(22.99682847336783, ' avgb', ' ', 'b', 3.1710464594368584)\n",
      "(23.004421325143383, ' avgv', ' ', 'v', 3.1710464594368584)\n",
      "(23.007955791262056, ' avgw', ' ', 'w', 3.1710464594368584)\n",
      "(23.01366459382852, ' avgx', ' ', 'x', 3.1710464594368584)\n",
      "(23.014225459578252, ' avgk', ' ', 'k', 3.1710464594368584)\n",
      "(23.0170443868546, ' avgq', ' ', 'q', 3.1710464594368584)\n",
      "(23.01829870056135, ' avgz', ' ', 'z', 3.1710464594368584)\n",
      "(23.018832507870243, ' avgj', ' ', 'j', 3.1710464594368584)\n",
      "(23.339400687051715, ' aw', 'vg ', 'w', 12.468128774344809)\n",
      "heap size is 0 after 99 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'average': 12.788721504012619}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('avg', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.387755905172256\n",
      "15.58013389926454\n",
      "4.807622005907717\n"
     ]
    }
   ],
   "source": [
    "lang_model=big_lang_m\n",
    "missed_model=big_err_m\n",
    "query = 'avg' + ' '\n",
    "prefix = ' '\n",
    "prefix_proba = 0.0\n",
    "freedom=3.0\n",
    "max_attempts=1000\n",
    "optimism=0.9\n",
    "verbose=True\n",
    "suffix = query\n",
    "full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "best_logprob = full_origin_logprob + no_missing_logprob\n",
    "print(best_logprob)\n",
    "print(full_origin_logprob)\n",
    "print(no_missing_logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'avg '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-lang_model.single_log_proba(prefix, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18.34898031465503, ' ', 'avg ', '', 18.34898031465503)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20.387755905172256, ' avg ', '', None, 0.0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{0: -0.0}\n",
      "a\n",
      "{0: -0.0, 1: 3.5233849549298424}\n",
      "av\n",
      "{0: -0.0, 1: 3.5233849549298424, 2: 8.651850082830299}\n",
      "avg\n",
      "{0: -0.0, 1: 3.5233849549298424, 2: 8.651850082830299, 3: 13.853476415938676}\n",
      "avg \n",
      "{0: -0.0, 1: 3.5233849549298424, 2: 8.651850082830299, 3: 13.853476415938676, 4: 17.941849393620302}\n"
     ]
    }
   ],
   "source": [
    "cache = {}\n",
    "for i in range(len(query)+1):\n",
    "    future_suffix = query[:i]\n",
    "    print(future_suffix)\n",
    "    cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "    #print(cache)\n",
    "    cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "    print(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.34898031465503, ' ', 'avg ', '', 18.34898031465503)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "next_best = heappop(heap)\n",
    "next_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg \n",
      "20.387755905172256\n"
     ]
    }
   ],
   "source": [
    "print(next_best[2])\n",
    "print(best_logprob)\n",
    "if next_best[2] == '':\n",
    "    print(next_best[2])\n",
    "    if next_best[0] <= best_logprob + freedom:\n",
    "        print(next_best)\n",
    "        #candidates.append(next_best)\n",
    "        # update the best likelihood\n",
    "        if next_best[0] < best_logprob:\n",
    "            print(next_best[0])\n",
    "            #best_logprob = next_best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if next_best[2] == '':  # it is a leaf\n",
    "    # this is the best leaf as far, add it to candidates\n",
    "    if next_best[0] <= best_logprob + freedom:\n",
    "        candidates.append(next_best)\n",
    "        # update the best likelihood\n",
    "        if next_best[0] < best_logprob:\n",
    "            best_logprob = next_best[0]\n",
    "else: # it is not a leaf - generate more options\n",
    "    prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "    prefix = next_best[1]\n",
    "    suffix = next_best[2]\n",
    "    new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "    # add only the solution potentioally no worse than the best + freedom\n",
    "    for new_option in new_options: \n",
    "        if new_option[0] < best_logprob + freedom:\n",
    "            heappush(heap, new_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_attempts):\n",
    "    if not heap:\n",
    "        break\n",
    "    next_best = heappop(heap)\n",
    "    if verbose:\n",
    "        print(next_best)\n",
    "    if next_best[2] == '':  # it is a leaf\n",
    "        # this is the best leaf as far, add it to candidates\n",
    "        if next_best[0] <= best_logprob + freedom:\n",
    "            candidates.append(next_best)\n",
    "            # update the best likelihood\n",
    "            if next_best[0] < best_logprob:\n",
    "                best_logprob = next_best[0]\n",
    "    else: # it is not a leaf - generate more options\n",
    "        prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "        prefix = next_best[1]\n",
    "        suffix = next_best[2]\n",
    "        new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "        # add only the solution potentioally no worse than the best + freedom\n",
    "        for new_option in new_options: \n",
    "            if new_option[0] < best_logprob + freedom:\n",
    "                heappush(heap, new_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_channel(word, lang_model, missed_model, freedom=1.0, max_attempts=1000, optimism=0.1, verbose=True):\n",
    "    query = word + ' '\n",
    "    prefix = ' '\n",
    "    prefix_proba = 0.0\n",
    "    suffix = query\n",
    "    full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "    no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "    best_logprob = full_origin_logprob + no_missing_logprob\n",
    "    # add empty beginning to the heap\n",
    "    heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "    # add the default option (no missing letters) to candidates\n",
    "    candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "    if verbose:\n",
    "        # todo: include distortion probability\n",
    "        print('baseline score is', best_logprob)\n",
    "    # prepare cache for suffixes (the slowest operation)\n",
    "    cache = {}\n",
    "    for i in range(len(query)+1):\n",
    "        future_suffix = query[:i]\n",
    "        cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "        cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "    \n",
    "    for i in range(max_attempts):\n",
    "        if not heap:\n",
    "            break\n",
    "        next_best = heappop(heap)\n",
    "        if verbose:\n",
    "            print(next_best)\n",
    "        if next_best[2] == '':  # it is a leaf\n",
    "            # this is the best leaf as far, add it to candidates\n",
    "            if next_best[0] <= best_logprob + freedom:\n",
    "                candidates.append(next_best)\n",
    "                # update the best likelihood\n",
    "                if next_best[0] < best_logprob:\n",
    "                    best_logprob = next_best[0]\n",
    "        else: # it is not a leaf - generate more options\n",
    "            prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "            prefix = next_best[1]\n",
    "            suffix = next_best[2]\n",
    "            new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "            # add only the solution potentioally no worse than the best + freedom\n",
    "            for new_option in new_options: \n",
    "                if new_option[0] < best_logprob + freedom:\n",
    "                    heappush(heap, new_option)\n",
    "    if verbose:\n",
    "        print('heap size is', len(heap), 'after', i, 'iterations')\n",
    "    result = {}\n",
    "    for candidate in candidates:\n",
    "        if candidate[0] <= best_logprob + freedom:\n",
    "            result[candidate[1][1:-1]] = candidate[0]\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
